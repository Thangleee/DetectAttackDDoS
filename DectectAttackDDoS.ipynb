{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "13LF3iyu-VUmf0Lsh66AaF1ec3uHJWE9Q",
      "authorship_tag": "ABX9TyO2V0KXxEWSkFQXX+/SfU9X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thangleee/DetectAttackDDoS/blob/main/DectectAttackDDoS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbqFWMpMzu5e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# For ploting the graphs\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import csv\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Machine learning Model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Machine learning model evaluation\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_curve, auc, confusion_matrix\n",
        "\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from itertools import cycle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FvDrW9KudxWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Đọc file\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/DA/data/DDos (1).csv\")"
      ],
      "metadata": {
        "id": "qydnCgvC0Lgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jVpPhAKmdwxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "ACiJNvS02MiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loai bo khoang trang ten cot\n",
        "df.columns = df.columns.str.strip()"
      ],
      "metadata": {
        "id": "vy5QkhlM2_c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unique values in the Label target column\n",
        "\n",
        "df.loc[:,'Label'].unique()"
      ],
      "metadata": {
        "id": "ewMvK1wFMrFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the null values in the dataset.\n",
        "plt.figure(1,figsize=( 10,4))\n",
        "plt.hist( df.isna().sum())\n",
        "# Set the title and axis labels\n",
        "plt.xticks([0, 1], labels=['Not Null=0', 'Null=1'])\n",
        "plt.title('Columns with Null Values')\n",
        "plt.xlabel('Feature')\n",
        "plt.ylabel('The number of features')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "P2ge3zOd2OOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plotMissingValues(dataframe):\n",
        "    missing_values = dataframe.isnull().sum()  # Counting null values for each column\n",
        "    fig = plt.figure(figsize=(16, 5))\n",
        "    missing_values.plot(kind='bar')\n",
        "    plt.xlabel(\"Features\")\n",
        "    plt.ylabel(\"Missing values\")\n",
        "    plt.title(\"Total number of Missing values in each feature\")\n",
        "    plt.show()\n",
        "\n",
        "plotMissingValues(df)"
      ],
      "metadata": {
        "id": "eBPXNw5aIoO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Removing the null values\n",
        "data_f=df.dropna()"
      ],
      "metadata": {
        "id": "FQd4KjTm2V0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the null values in the dataset.\n",
        "plt.figure(1,figsize=( 10,4))\n",
        "plt.hist( data_f.isna().sum())\n",
        "# Set the title and axis labels\n",
        "plt.title('Data aftter removing the Null Values')\n",
        "plt.xlabel('The number of null values')\n",
        "plt.ylabel('Number of columns')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Vc9bHzGv3H2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('use_inf_as_na', True)  # Treat inf as NaN\n",
        "null_values=data_f.isnull().sum()  # Check for NaN values"
      ],
      "metadata": {
        "id": "CyelrU4N3Ks0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To know the data types of the columns\n",
        "\n",
        "(data_f.dtypes=='object')"
      ],
      "metadata": {
        "id": "PZz-bGiS3L9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the labels in the DataFrame to numerical values\n",
        "data_f['Label'] = data_f['Label'].map({'BENIGN': 0, 'DDoS': 1})"
      ],
      "metadata": {
        "id": "Ydtp12C0OZd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the DataFrame\n",
        "\n",
        "plt.hist(data_f['Label'], bins=[0, 0.3,0.7,1], edgecolor='black')  # Specify bins as [0, 1]\n",
        "plt.xticks([0, 1], labels=['BENIGN=0', 'DDoS=1'])\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fIyPZGWo3iGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Đếm số lượng từng lớp\n",
        "label_counts = data_f['Label'].value_counts()\n",
        "labels = ['Benign (0)', 'DDoS (1)']\n",
        "sizes = [label_counts[0], label_counts[1]]\n",
        "colors = ['skyblue', 'lightcoral']\n",
        "\n",
        "# Vẽ biểu đồ tròn\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Tỷ lệ giữa các lớp trong tập dữ liệu')\n",
        "plt.axis('equal')  # Đảm bảo hình tròn\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6NnNE7khzMkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into features and target variable\n",
        "X = data_f.drop('Label', axis=1)\n",
        "y = data_f['Label']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
      ],
      "metadata": {
        "id": "ztXt_Ujz3kld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "rWmX1I8A0IN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The train dataset size = \",X_train.shape)\n",
        "print(\"The test dataset size = \",X_test.shape)"
      ],
      "metadata": {
        "id": "gzGCwJ-zK1Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_test)"
      ],
      "metadata": {
        "id": "nQ2cQZDqLZFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting feature importances from the trained model\n",
        "importances = rf_model.feature_importances_\n",
        "\n",
        "# Getting the indices of features sorted by importance\n",
        "indices = sorted(range(len(importances)), key=lambda i: importances[i], reverse=False)\n",
        "feature_names = [f\"Features {i}\" for i in indices]  # Replace with your column names\n",
        "\n",
        "# Plotting feature importances horizontally\n",
        "plt.figure(figsize=(8, 14))\n",
        "plt.barh(range(X_train.shape[1]), importances[indices], align=\"center\")\n",
        "plt.yticks(range(X_train.shape[1]), feature_names)\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.title(\"Feature Importances\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_AnmpOFG4ZYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import plot_tree\n",
        "\n",
        "estimator = rf_model.estimators_[0]  # Selecting the first estimator from the random forest model\n",
        "\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(estimator, filled=True, rounded=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GIRBxB6N4a7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate and display a detailed confusion matrix\n",
        "def plot_confusion_matrix(y_true, y_pred, classes, title):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "eauJuZALGlnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Random Forest\n",
        "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
        "rf_f1 = f1_score(y_test, rf_pred)\n",
        "rf_precision = precision_score(y_test, rf_pred)\n",
        "rf_recall = recall_score(y_test, rf_pred)\n",
        "\n",
        "print('\\nRandom Forest Metrics:')\n",
        "print(f'Accuracy: {rf_accuracy:.4f}')\n",
        "print(f'F1 Score: {rf_f1:.4f}')\n",
        "print(f'Precision: {rf_precision:.4f}')\n",
        "print(f'Recall: {rf_recall:.4f}')"
      ],
      "metadata": {
        "id": "4XWAzhsGGpMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix for Random Forest\n",
        "plot_confusion_matrix(y_test, rf_pred, ['Benign', 'DDoS'], 'Random Forest Confusion Matrix')"
      ],
      "metadata": {
        "id": "CEsrB20oGq3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression"
      ],
      "metadata": {
        "id": "TnfNGFh40gq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model = LogisticRegression(random_state=42, max_iter=100)\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "lr_pred = lr_model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "WDTy7_PMQrRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
        "lr_f1 = f1_score(y_test, lr_pred)\n",
        "lr_precision = precision_score(y_test, lr_pred)\n",
        "lr_recall = recall_score(y_test, lr_pred)\n",
        "\n",
        "print('\\nLogistic Regression Metrics:')\n",
        "print(f'Accuracy: {lr_accuracy:.4f}')\n",
        "print(f'F1 Score: {lr_f1:.4f}')\n",
        "print(f'Precision: {lr_precision:.4f}')\n",
        "print(f'Recall: {lr_recall:.4f}')"
      ],
      "metadata": {
        "id": "qgwwu0QHQs8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix for Logistic Regression\n",
        "plot_confusion_matrix(y_test, lr_pred, ['Benign', 'DDoS'], 'Logistic Regression Confusion Matrix')"
      ],
      "metadata": {
        "id": "ZwxD2iIXQydx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Neural Network"
      ],
      "metadata": {
        "id": "A2_4fMlF00eO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn_model = MLPClassifier(hidden_layer_sizes=(10,), max_iter=100, random_state=42)\n",
        "nn_model.fit(X_train_scaled, y_train)\n",
        "nn_pred = nn_model.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "mi4CsKJ_Q2Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_accuracy = accuracy_score(y_test, nn_pred)\n",
        "nn_f1 = f1_score(y_test, nn_pred)\n",
        "nn_precision = precision_score(y_test, nn_pred)\n",
        "nn_recall = recall_score(y_test, nn_pred)\n",
        "\n",
        "print('\\nNeural Network Metrics:')\n",
        "print(f'Accuracy: {nn_accuracy:.4f}')\n",
        "print(f'F1 Score: {nn_f1:.4f}')\n",
        "print(f'Precision: {nn_precision:.4f}')\n",
        "print(f'Recall: {nn_recall:.4f}')"
      ],
      "metadata": {
        "id": "P7HAsgPiQ4WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix for Neural Network\n",
        "plot_confusion_matrix(y_test, nn_pred, ['Benign', 'DDoS'], 'Neural Network Confusion Matrix')"
      ],
      "metadata": {
        "id": "kFN1f7NnQ6tm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest\n",
        "rf_proba = rf_model.predict_proba(X_test)\n",
        "\n",
        "# Logistic Regression\n",
        "lr_proba = lr_model.predict_proba(X_test_scaled)\n",
        "\n",
        "\n",
        "\n",
        "# Neural Network\n",
        "nn_proba = nn_model.predict_proba(X_test_scaled)"
      ],
      "metadata": {
        "id": "Fq4QVJp6Q-uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine predictions for ROC curve\n",
        "\n",
        "# Calculate ROC curve for Random Forest\n",
        "rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_proba[:, 1])\n",
        "rf_auc = auc(rf_fpr, rf_tpr)\n",
        "\n",
        "# Calculate ROC curve for Logistic Regression\n",
        "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_proba[:, 1])\n",
        "lr_auc = auc(lr_fpr, lr_tpr)\n",
        "\n",
        "# Calculate ROC curve for Neural Network\n",
        "nn_fpr, nn_tpr, _ = roc_curve(y_test, nn_proba[:, 1])\n",
        "nn_auc = auc(nn_fpr, nn_tpr)"
      ],
      "metadata": {
        "id": "jp3DZIyDRA7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ROC curves for all models\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(rf_fpr, rf_tpr, label=f'Random Forest (AUC = {rf_auc:.2f})')\n",
        "plt.plot(lr_fpr, lr_tpr, label=f'Logistic Regression (AUC = {lr_auc:.2f})')\n",
        "plt.plot(nn_fpr, nn_tpr, label=f'Neural Network (AUC = {nn_auc:.2f})')\n",
        "\n",
        "# Plot ROC curve for random classifier (50% area)\n",
        "plt.plot([0, 1], [0, 1], linestyle='--', color='black', label='Random Classifier (AUC = 0.50)')\n",
        "\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GOkepvwC1Yj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Lưu mô hình vào file\n",
        "joblib.dump(rf_model, '/content/drive/MyDrive/DATN/random_forest_model.pkl')\n",
        "\n",
        "joblib.dump(scaler, '/content/drive/MyDrive/DATN/scaler.pkl')\n"
      ],
      "metadata": {
        "id": "Jx_O9hK3aTUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = df.columns.drop('Label')  # hoặc danh sách cụ thể nếu muốn kiểm soát\n",
        "\n",
        "# Lưu lại để dùng về sau\n",
        "joblib.dump(features, '/content/drive/MyDrive/DATN/features.pkl')\n"
      ],
      "metadata": {
        "id": "Fggtf9dm5pQl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}